{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning-neural-network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDrWSb5dcYUwIJKG5WMQFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshayparanjape/Data_Structure_assignments/blob/master/Q_learning_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc"
      ],
      "metadata": {
        "id": "8izotlKQa-PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-Learning, Deep Q-Networks, and Policy Gradient methods are model-free algorithms "
      ],
      "metadata": {
        "id": "lsD7j0npZgkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanilla Q-learning: A table maps each state-action pair to its corresponding Q-value\n",
        "\n",
        "Deep Q-Learning: A Neural Network maps input states to (action, Q-value) pairs"
      ],
      "metadata": {
        "id": "nuXgqXzqagyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Q-Learning replaces the regular Q-table with a neural network. Rather than mapping a state-action pair to a q-value, a neural network maps input states to (action, Q-value) pairs"
      ],
      "metadata": {
        "id": "-0ADuxFda7-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "TGAIpRDEdQW3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1USWhpRsXLZu"
      },
      "outputs": [],
      "source": [
        "def agent(state_shape, action_shape):\n",
        "    learning_rate = 0.001\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experience Replay is the act of storing and replaying game states (the state, action, reward, next_state) that the RL algorithm is able to learn from\n",
        "\n",
        "Temporal Difference part of Bellman equation\n",
        "\n",
        "Note that the target network and not the main network is used to calculate the Temporal Difference target."
      ],
      "metadata": {
        "id": "1_ibRJXOdhy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete Implementation"
      ],
      "metadata": {
        "id": "YA5J1Zpte0Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A Minimal Deep Q-Learning Implementation (minDQN)\n",
        "Running this code will render the agent solving the CartPole environment using OpenAI gym. Our Minimal Deep Q-Network is approximately 150 lines of code. In addition, this implementation uses Tensorflow and Keras and should generally run in less than 15 minutes.\n",
        "Usage: python3 minDQN.py\n",
        "\"\"\"\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import deque\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "YOIIys4re5xE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 5\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "49DnrwWfe_fY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Action Space: {}\".format(env.action_space))\n",
        "print(\"State space: {}\".format(env.observation_space))\n",
        "\n",
        "# An episode a full game\n",
        "train_episodes = 300\n",
        "test_episodes = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7E59fIdfGZh",
        "outputId": "927b5c63-d424-4759-d84a-6e8bf8e51190"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "State space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def agent(state_shape, action_shape):\n",
        "    \"\"\" The agent maps X-states to Y-actions\n",
        "    e.g. The neural network output is [.1, .7, .1, .3]\n",
        "    The highest value 0.7 is the Q-Value.\n",
        "    The index of the highest action (0.7) is action #1.\n",
        "    \"\"\"\n",
        "    learning_rate = 0.001\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_qs(model, state, step):\n",
        "    return model.predict(state.reshape([1, state.shape[0]]))[0]"
      ],
      "metadata": {
        "id": "pqr1yoDgf1-d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, replay_memory, model, target_model, done):\n",
        "    learning_rate = 0.7 # Learning rate\n",
        "    discount_factor = 0.618\n",
        "\n",
        "    MIN_REPLAY_SIZE = 1000\n",
        "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
        "        return\n",
        "\n",
        "    batch_size = 64 * 2\n",
        "    mini_batch = random.sample(replay_memory, batch_size)\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    current_qs_list = model.predict(current_states)\n",
        "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
        "    future_qs_list = target_model.predict(new_current_states)\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
        "        if not done:\n",
        "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
        "        else:\n",
        "            max_future_q = reward\n",
        "\n",
        "        current_qs = current_qs_list[index]\n",
        "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
        "\n",
        "        X.append(observation)\n",
        "        Y.append(current_qs)\n",
        "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
      ],
      "metadata": {
        "id": "ymYaEn7rf6F3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
        "    max_epsilon = 1 # You can't explore more than 100% of the time\n",
        "    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n",
        "    decay = 0.01\n",
        "\n",
        "    # 1. Initialize the Target and Main models\n",
        "    # Main Model (updated every 4 steps)\n",
        "    model = agent(env.observation_space.shape, env.action_space.n)\n",
        "    # Target Model (updated every 100 steps)\n",
        "    target_model = agent(env.observation_space.shape, env.action_space.n)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "\n",
        "    replay_memory = deque(maxlen=50_000)\n",
        "\n",
        "    target_update_counter = 0\n",
        "\n",
        "    # X = states, y = actions\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    steps_to_update_target_model = 0\n",
        "\n",
        "    for episode in range(train_episodes):\n",
        "        total_training_rewards = 0\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            steps_to_update_target_model += 1\n",
        "            if True:\n",
        "                env.render()\n",
        "\n",
        "            random_number = np.random.rand()\n",
        "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "            if random_number <= epsilon:\n",
        "                # Explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # Exploit best known action\n",
        "                # model dims are (batch, env.observation_space.n)\n",
        "                encoded = observation\n",
        "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
        "                predicted = model.predict(encoded_reshaped).flatten()\n",
        "                action = np.argmax(predicted)\n",
        "            new_observation, reward, done, info = env.step(action)\n",
        "            replay_memory.append([observation, action, reward, new_observation, done])\n",
        "\n",
        "            # 3. Update the Main Network using the Bellman Equation\n",
        "            if steps_to_update_target_model % 4 == 0 or done:\n",
        "                train(env, replay_memory, model, target_model, done)\n",
        "\n",
        "            observation = new_observation\n",
        "            total_training_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n",
        "                total_training_rewards += 1\n",
        "\n",
        "                if steps_to_update_target_model >= 100:\n",
        "                    print('Copying main network weights to the target network weights')\n",
        "                    target_model.set_weights(model.get_weights())\n",
        "                    steps_to_update_target_model = 0\n",
        "                break\n",
        "\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "2xEKXrgIafYC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "jCjRN4qVhmZW",
        "outputId": "241cc8c1-4b2a-4f3c-b40a-0f2bcfc2af62"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-68c18df366e3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0msteps_to_update_target_model\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mrandom_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UHAABgfJj7YP"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}